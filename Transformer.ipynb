{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPJwiQBGGDJv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "  def __init__(self,d_model,vocab_size):\n",
        "    super().__init__()\n",
        "    self.vocab_size=vocab_size\n",
        "    self.d_model=d_model\n",
        "    self.embeddings=nn.Embedding(vocab_size,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embeddings(x)*torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "\n",
        "# sentence='the animal is too tired'\n",
        "# vocab_size=len(sentence.split(' '))\n",
        "# m=InputEmbeddings(10,vocab_size)\n",
        "# tokens=sentence.split(' ')\n",
        "\n",
        "# word_to_id = {word: i for i, word in enumerate(tokens)}\n",
        "# input_indices = torch.tensor([word_to_id[w] for w in tokens], dtype=torch.long)\n",
        "# embeddings = m(input_indices)\n",
        "# len(embeddings)\n",
        "# input_indices\n",
        "# embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class Positional_Encoding(nn.Module):\n",
        "  def __init__(self,d_model,max_len,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.seq_len=max_len\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    pos_matrix=torch.zeros(self.seq_len,self.d_model)\n",
        "\n",
        "    position=torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    div_term=torch.exp(\n",
        "        torch.arange(0,d_model,2).float()*(-math.log(10000.0)/d_model)\n",
        "    )\n",
        "\n",
        "    pos_matrix[:,0::2]=torch.sin(position*div_term)\n",
        "    pos_matrix[:,1::2]=torch.cos(position*div_term)\n",
        "\n",
        "    pos_matrix=pos_matrix.unsqueeze(0)\n",
        "    self.register_buffer('pos_matrix',pos_matrix)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=x+(self.pos_matrix[:,:x.shape[1],:]).requires_grad_(False)\n",
        "\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "PEHSuzy59TF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.heads = heads\n",
        "        self.d_k = d_model // heads\n",
        "\n",
        "        assert d_model % heads == 0\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.output = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        weights = scores.softmax(dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            weights = dropout(weights)\n",
        "\n",
        "        return (weights @ value), weights\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        query = self.query(q)\n",
        "        key = self.key(k)\n",
        "        value = self.value(v)\n",
        "\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attention_weights = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.d_model)\n",
        "\n",
        "        return self.output(x)\n",
        "\n",
        "# --- Test Example ---\n",
        "\n",
        "# d_model = 512\n",
        "# heads = 8\n",
        "# seq_len = 10\n",
        "# batch_size = 2\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, heads)\n",
        "# x = torch.randn(batch_size, seq_len, d_model)\n",
        "# mask = torch.ones(batch_size, 1, 1, seq_len)\n",
        "\n",
        "# out = mha(x, x, x, mask)\n",
        "# print(out.shape)\n"
      ],
      "metadata": {
        "id": "-Cd7h72rZtx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "a=nn.Linear(3,2)\n"
      ],
      "metadata": {
        "id": "Q-eIA8JDff6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class PositionFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,d_diff,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.w1=nn.Linear(d_model,d_diff)\n",
        "\n",
        "\n",
        "    self.w2=nn.Linear(d_diff,d_model)\n",
        "\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    self.relu=nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    return self.w2(self.dropout(self.relu(self.w1(x))))\n",
        "\n"
      ],
      "metadata": {
        "id": "77OjZFfpce6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    # 1. We add 'features' (d_model) so we can size our parameters correctly\n",
        "    def __init__(self, features: int, eps: float = 10**-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "        # 2. We make alpha and bias a vector of size 512!\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        # Pro-tip: Standard LayerNorm doesn't use Bessel's correction (unbiased=False)\n",
        "        std = x.std(dim=-1, unbiased=False, keepdim=True)\n",
        "\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "Jjfcniu9fNy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Self-Attention Sub-layer\n",
        "        self.self_attention = MultiHeadAttention(d_model, heads, dropout)\n",
        "        self.norm1 = LayerNormalization(d_model) # Using PyTorch's built-in LayerNorm for stability\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # 2. Feed-Forward Sub-layer\n",
        "        self.feed_forward = PositionFeedForward(d_model, d_ff, dropout)\n",
        "        self.norm2 = LayerNormalization(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x shape: (Batch, Seq_Len, d_model)\n",
        "        # mask shape: (Batch, 1, 1, Seq_Len)\n",
        "\n",
        "        # --- Sub-layer 1: Attention ---\n",
        "        # 1. Calculate Attention\n",
        "        # Note: In Encoder, Query=Key=Value=x\n",
        "        attention_out = self.self_attention(x, x, x, mask)\n",
        "\n",
        "        # 2. Add & Norm\n",
        "        # We add the ORIGINAL input 'x' to the attention output (Residual Connection)\n",
        "        x = self.norm1(x + self.dropout1(attention_out))\n",
        "\n",
        "        # --- Sub-layer 2: Feed Forward ---\n",
        "        # 3. Calculate Feed Forward\n",
        "        ff_out = self.feed_forward(x)\n",
        "\n",
        "        # 4. Add & Norm\n",
        "        # We add the input of this sub-layer (which is the output of norm1) to the FF output\n",
        "        x = self.norm2(x + self.dropout2(ff_out))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "BrQBPMrTgBQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.N = N\n",
        "\n",
        "        # 1. Embeddings & Positional Encoding\n",
        "        self.embed = InputEmbeddings(d_model, vocab_size)\n",
        "        self.pe = Positional_Encoding(d_model, max_len, dropout)\n",
        "\n",
        "        # 2. The Stack of N Encoder Layers\n",
        "        # We use nn.ModuleList to store a list of layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderBlock(d_model, heads, d_ff, dropout)\n",
        "            for _ in range(N)\n",
        "        ])\n",
        "\n",
        "        # 3. Final Normalization Layer\n",
        "        self.norm = LayerNormalization(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        # src shape: (Batch, Seq_Len)\n",
        "\n",
        "        # 1. Embed and Add Position\n",
        "        x = self.embed(src) # (Batch, Seq_Len) -> (Batch, Seq_Len, d_model)\n",
        "        x = self.pe(x)\n",
        "\n",
        "        # 2. Pass through each of the N layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # 3. Final Norm\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "q783gx0sgF6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoder**"
      ],
      "metadata": {
        "id": "EsIe06u8QH46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Instantiate the layers inside the block (The Modern Way!)\n",
        "        self.self_attention = MultiHeadAttention(d_model, heads, dropout)\n",
        "        self.cross_attention = MultiHeadAttention(d_model, heads, dropout)\n",
        "        self.feed_forward = PositionFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = LayerNormalization(d_model)\n",
        "        self.norm2 = LayerNormalization(d_model)\n",
        "        self.norm3 = LayerNormalization(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        # 1. Masked Self-Attention\n",
        "        _x = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(_x))\n",
        "\n",
        "        # 2. Cross-Attention (Query from Decoder 'x', Key/Value from 'encoder_output')\n",
        "        _x = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(_x))\n",
        "\n",
        "        # 3. Feed Forward\n",
        "        _x = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(_x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ewxjiBRpQOa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.N = N\n",
        "\n",
        "        # 1. Embeddings & Positional Encoding for Target Language\n",
        "        self.embed = InputEmbeddings(d_model, vocab_size)\n",
        "        self.pe = Positional_Encoding(d_model, max_len, dropout)\n",
        "\n",
        "        # 2. The Stack of N Decoder Layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, heads, d_ff, dropout)\n",
        "            for _ in range(N)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, encoder_output, src_mask, tgt_mask):\n",
        "        # 1. Embed and Add Position\n",
        "        x = self.embed(tgt)\n",
        "        x = self.pe(x)\n",
        "\n",
        "        # 2. Pass through each of the N layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "rFMsDHk4HwGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.proj(x), dim=-1)"
      ],
      "metadata": {
        "id": "shrXA0WfH1sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, N=6, heads=8, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Build the entire Encoder stack\n",
        "        self.encoder = TransformerEncoder(src_vocab_size, d_model, N, heads, d_ff, dropout)\n",
        "\n",
        "        # Build the entire Decoder stack\n",
        "        self.decoder = TransformerDecoder(tgt_vocab_size, d_model, N, heads, d_ff, dropout)\n",
        "\n",
        "        # Build the final word generator\n",
        "        self.generator = Generator(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        # Pass input through Encoder\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "\n",
        "        # Pass target and encoder output through Decoder\n",
        "        decoder_output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Generate word probabilities\n",
        "        return self.generator(decoder_output)"
      ],
      "metadata": {
        "id": "io3pHsGMJjUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}